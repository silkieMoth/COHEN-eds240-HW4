---
title: "Bias against spiders in the news and what we can do about it"
author: "Joshua Paul Cohen"
format: html
execute: 
  eval: false
  echo: true
---

# Spiders, do you fear them?

People’s fear of the wilds is largely motivated by a broader fear of the unknown. But we can’t ignore the space outside our cities, because it *will* find its way inside…mainly the spider eyeballing you in the corner of your room right now.

I try not to judge people who are afraid of spiders, because there was a point in my life where I was afraid of them too. When you’re a kid, you learn about spiders through movies and TV, and once you’re taught to fear them the adults around you condition you to reinforce that feeling. But as I came of age, I decided that I didn’t want to be afraid anymore. And in that process, I realized that maybe I shouldn’t have been afraid to begin with. I learned that spiders aren’t the aggressive threat I thought they were and are just trying to live their lives just like we are. Only that the scope of their world is inconceivably more terrifying than ours. In fact, I learned that most spiders I’ll encounter are incapable of causing me much harm at all. So what taught me to be so scared of them? 

I think that, when you’re young, you are mainly taught to think what other people think. And other people watched the news.

The bias in reporting of ecological topics is less talked of in the broader discussion of mainstream media bias, but it’s very prevalent. This is because generally as a society, we just like large and charismatic mammals, and dislike bugs. The media we consume is just a reflection of these attitudes, and the cycle continues.

My since my awakening per say, I’ve been trying to think of how I could help other people break down this fear. A tall order I know, since these deep-seeded and long-lived fears persist for many people. But that won’t stop me from trying. 

I stumbled upon a dataset by Mammola et al. (2022) about the very thing I wish people would think more about. It is tabular compilation of news articles from around the world all about spiders, each row containing detailed info about what biases it may contain. It was essentially direct evidence of the problem, so I made like a jumping spider and pounced on the opportunity to visualize it. 

The dataset had a lot of information and allowed many possible questions to answer. At first I thought maybe I would visualize bias by country, or maybe bias by year. But the reality is that I just want you, reader, to think about how your impression of spiders might not even be your own. I have no delusions that you will overcome your fear just by reading this, doing so is a whole process. But my goal for you maybe, is to consider if you should.


## Infographic

![](infographic_edit_4x.png)

One of the ways the news tricks you is framing. Framing simply put is the way that information is presented. This could include aesthetics, word choice, or maybe even selectively picking bits of said info that suit a narrative. Framing isn't necessarily a bad thing, sometimes the news we hear *should* be framed in a positive or negative light. But decisions to do so can also be harmful.

Spiders are a group of critters that are frequently hurt by this. They are frequently framed in a negative light because of stigma and playing into it increases website traffic and add revenue, but also because people tend to only think about spiders when the interaction is negative. That being said, we can try to paint spiders in a more positive light with the design choices of this infographic.

Design elements can be reduced down to general structure (graphic form), aesthetic elements (themes, colors, typography), direct messaging (titles, annotations, providing context), and DEI considerations (accessibility, equity, etc).

With the aesthetic elements, there's a fine line between glamorization and feeding into the stigma. I chose a light grey and matte color scheme because this is a relatively neutral theme and colorization, while also keeping the spider-esque theme. The fonts I've chosen are either thematic or keep a casual, neutral tone.

The message of my infographic is pretty clear, that the stigma surround spiders is unfair to them, and the actual problem lies within ourselves. I accomplish this many ways. My sentiment analysis graph shows that public opinion in the news is predominantly negative by a pretty vast margin. 

My top chart shows that there is actually a disproportionate amount of spiders in the news that are not medically relevant. While this in of itself is not indicative of bias, it is somewhat common for non-medicaly relevant spiders to be misidentified as being so. I also calculate the bias between the two categories. For each article, the dataset contained several binary variables categorizing different kinds of biases, such as the inclusion of an expert opinion, sensationalism, reporting errors, and other miscellaneous possible bias prone characteristics such as if a death was reported or a picture of a spider bite was included. In short, to derive a bias score from these variables I arbitrarily assigned a weight to each of them based on how damaging I think the bias is. Lastly, I also characterize the distribution of news articles by family. 

My last visualization in the bottom left corner, motivates some solutions. Karner et al. (2024) conducted a "exposure therapy" study on over 200 subjects, where they were shown visual images of spiders. Throughout the study, they filled out the Fear of Spiders Questionnaire (FSQ) as developed by Szymanski and O'Donohue (1995). It was filled out before, during, and after the study for a total of 3 trials. I chose to visualize some of the results of this study, because it's important that we not just talk about spider bias, but also actively work towards solving the problem. I've also included a quote that I think highlights my take home message.

For accessibility, I believe the color scheme is colorblind friendly. There is perhaps some DEI context in regards to how opportunities to experience nature are unequal along socioeconomic lines, which is itself worthwhile to explore, it just wasn't the focus of this project.

## Code

#### Code from this project can be view by expanding the following chunks.

Loading libraries and data
```{r}
#| code-fold: true

# libraries
library(tidyverse)
library(syuzhet)
library(lubridate)
library(tm)
library(wordcloud)
library(tidytext)
library(textdata)
library(patchwork)
library(polyglotr)

# --------------load_fonts-------------------
extrafont::loadfonts("win", quiet = FALSE)

# --------------load_data---------------------
spider_news <- read_delim(here::here('data', 'Data_spider_news_global.csv'), delim = '\t')

spider_tolerance <- read_delim(here::here("data", "SpiDa", "SpiDa_filtered.csv"), delim = ";")
```

Calculating bias score
```{r}
#| code-fold: true
  

##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##                                                                            --
##------------------------------ WEIGH THE BIAS---------------------------------
##                                                                            --
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

spider_news_weighted <- spider_news %>% 
  
  # account for Nas
  mutate(across(Bite:Photo_error, ~ replace_na(.,1))) %>% 
  
  # impacts to bias score assigned arbitrarily
  mutate(
    
         Bite = Bite * 1,
         Death = Death * 1, 
         Figure_species = Figure_species * 1, 
         Figure_bite = Figure_bite * 2, 
         
         # having an expert will reduce bias score
         Expert_arachnologist = Expert_arachnologist * -2,
         Expert_doctor = Expert_doctor * -1,
         Expert_others = Expert_others * -1, 
         
         # sensationalism give sever bias penalty
         Sensationalism = Sensationalism * 5,
         
         # multiple types of error will compound
         Taxonomic_error = case_when(
           Taxonomic_error != 0 ~ Taxonomic_error * 2, 
           Taxonomic_error == 0 | is.na(Taxonomic_error) == TRUE ~ 1),
         Venom_error = case_when(
           Venom_error != 0 ~ Venom_error * 2, 
           Venom_error == 0 | is.na(Venom_error) == TRUE ~ 1),
         Anatomy_error = case_when(
           Anatomy_error != 0 ~ Anatomy_error * 2, 
           Anatomy_error == 0 | is.na(Anatomy_error) == TRUE ~ 1),
         Photo_error = case_when(
           Photo_error != 0 ~ Photo_error * 2, 
           Photo_error == 0 | is.na(Photo_error) == TRUE ~ 1),
         
         Total_error = Bite + Death + Figure_species + Expert_arachnologist + Expert_doctor + Expert_others + (Taxonomic_error * Venom_error * Anatomy_error * Photo_error)
         
         )
```

Building translation function
```{r}
#| code-fold: true

# function translates a vector of strings to english
translate_col <- function(col, j, k){

# chunk size
n_iter <- 50

# split vector into chunks
chunks <- split(col, cut(seq_along(col), length(col)/n_iter, labels=FALSE))

# init empty vector for translated news titles
translations <- c()

# inits the loading par
pb <- txtProgressBar(min = j,      # Minimum value of the progress bar
                     max = k, # Maximum value of the progress bar
                     style = 3,    # Progress bar style
                     width = 50,   # Progress bar width. 
                     char = "=")   # Character used to create the bar



for(i in j:k) {

    #---------------------
    # Code to be executed
    #---------------------
  
  # error handling, will pass chunk if
  translations <- tryCatch(
    expr = {
  
  # translate chunk
   translated <- google_translate(unlist(chunks[i]))
   
   # append translated chunk to rest of them
   translations <- rbind(translations, data.frame(x = matrix(translated, nrow=132, byrow=TRUE),stringsAsFactors=FALSE))
   
    },
   # will just return NA if error
    error = function(e) {
      paste(NA)
    }
  )
  
    #---------------------

    # Sets the progress bar to the current state
    setTxtProgressBar(pb, i)
   
  # waits 5 seconds before starting next chunk, to avoid overloading servers
   Sys.sleep(5)
}

return(translations)

close(pb) # ends progress bar

}
```


Running translation function
```{r}
#| code-fold: true

# Reason for splitting titles into chunks and running function multiple times is because of a "malformed URL" error. Possible causes of error:
# 
#  `google_translate()` is being passed strings containing characters or sequences of characters that are incompatible.
#  `google_translate()` struggles to handle this large of a dataset.


# split news titles into chunks
chunks <- split(spider_news$Title, cut(seq_along(spider_news$Title), 124, labels=FALSE))

# translate first half of titles
news_translated_1 <- translate_col(spider_news$Title, 1, length(chunks)/2)

# translate third quarter
news_translated_2 <- translate_col(Filter(function(x) x != "No title", spider_news$Title), length(chunks)/2 + 1, (length(chunks)/2) + 27)

# translate some more
news_translated_3 <- translate_col(Filter(function(x) x != "No title", spider_news$Title), (length(chunks)/2) + 28, (length(chunks)/2) + 40)

# translate rest
news_translated_4 <- translate_col(Filter(function(x) x != "No title", spider_news$Title), (length(chunks)/2) + 41, length(chunks))

full_translations <- rbind(news_translated_1, news_translated_2, news_translated_3, news_translated_4)
```

Sentiment analysis processing
```{r}
#| code-fold: true

##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##                             try sentiment analysis                      ----
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


# create corpus
title_corpus <- full_translations$x %>% 
  as_vector() %>% VectorSource() %>% 
  SimpleCorpus()

# remove characters/attributes that would distrupt analysis
title_corpus_sentiment <- title_corpus %>% 
  tm_map(tolower) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(removeNumbers) %>% 
  tm_map(removeWords, stopwords('english')) %>% 
  tm_map(stripWhitespace)

# turn corpus into matrix
tdm <- TermDocumentMatrix(title_corpus_sentiment) %>% as.matrix()

# tabulate counts of all words
row_sums <- rowSums(tdm)

# convert counts to tibble
sentiment_tib <- enframe(row_sums, name = "word", value = "count")


# Convert to data frame (same as above)
afinn_prep_df <- as_data_frame(full_translations) %>% 
  rename('text' = 'x')

# Convert to lowercase (same as above)
afinn_prep_df$text <- tolower(afinn_prep_df$text)

# Remove punctuation (same as above)
afinn_prep_df$text <- gsub("[[:punct:]]", "", afinn_prep_df$text)

# Unnest the text into words (same as above)
text_words <- afinn_prep_df %>%
  tidytext::unnest_tokens(word, text)

# depluralize words
deplural <- text_words %>% 
  mutate(word = pluralize::singularize(word))



```

Wordcloud preparation
```{r}
#| code-fold: true

# prepare new wordcloud df
wordcloud_prep <- data.frame(names(row_sums), as_tibble(row_sums)) %>% 
  rename(word = names.row_sums., frequency = value) %>%
  
  # make all plural words singular
  
  mutate(word = pluralize::singularize(word)) %>% 
  
  # sum all frequencies of words
  group_by(word) %>% reframe(frequency = sum(frequency)) %>% 
  
  # omit "spider" as it is common and reduntant
  filter(!word %in% c("spider")) %>% 
  inner_join(afinn, by = "word")
```

Building wordcloud (present in a previous version of the infographic)
```{r}
#| code-fold: true

# init save png wrapper
png("figs/sent_wordcloud.png", width = 1000, height = 1000, res = 300, bg = "transparent")

wordcloud(words = wordcloud_prep$word,
          freq = wordcloud_prep$frequency,
          max.words = 75,
          random.order = TRUE, # order in which words are plotted is random
          min.freq = 5,
          colors = tail(brewer.pal(9, 'Purples'), 5),
          scale = c(3, 0.5), # size difference between high a low count words
          rot.per = 0, # all words horizontal
          use.r.layout = FALSE, 
          family = "Kirang Haerang",)

# end of png wrapper
dev.off()
```

More sentiment analysis processing (NRC)
```{r, fig.asp=1.5}
#| code-fold: true

#...................Try NRC Sentiment Analysis...................

# get nrc dictionary
nrc <- get_sentiments("nrc")

# Join the text words with the NRC lexicon
sentiment_analysis_nrc <- text_words %>%
  inner_join(nrc, by = "word")

# Summarize sentiment counts
sentiment_summary_nrc <- sentiment_analysis_nrc %>%
  count(sentiment, sort = TRUE)
```
Building NRC sentiment analysis graph
```{r}
#| code-fold: true

# Create a bar chart of sentiment counts
nrc_plot <- ggplot() +
  
  # init bars
  geom_bar(aes(x = reorder(sentiment, n), 
               y = n*10^-3, 
               fill = sentiment), 
           sentiment_summary_nrc %>% 
             filter(!sentiment %in% c("positive", "negative")), 
           stat = "identity", show.legend = FALSE) +
  
  # add count labels
  geom_text(aes(x = sentiment, 
                y = n*10^-3 + 5, 
                label = round(n*10^-3, 1)), 
            sentiment_summary_nrc %>% filter(!sentiment %in% c("positive", "negative")), 
            family = "Gaegu", 
            fontface = "bold") + 
  
  # add sentiment labels
  geom_text(aes(x = sentiment, 
                y = n*10^-3 + 25, 
                label = sentiment, 
                angle = txt_angle_val), 
            sentiment_summary_nrc %>% filter(!sentiment %in% c("positive", "negative")), 
            family = "Gaegu", 
            fontface = "bold", 
            size = 4) + 
  
  # make plot radial
  coord_radial(inner.radius = 0.05,
             start = -1.2*pi, end = 0.2*pi, # sets 3/5 circle about,
             ) + 
  
  # make sequential color scale
  scale_fill_manual(values = scales::pal_seq_gradient("black", "khaki3")(seq(0,1, length.out = 8))) + 
  
  # thematic elements
  theme_minimal() + 
  theme(
    axis.title = element_blank(),
    plot.title = element_text(family = "Ink Free", face = "bold"), 
    panel.grid = element_blank(), 
    axis.text = element_blank()
  )

  nrc_plot <- cowplot::ggdraw(nrc_plot) +
  cowplot::draw_text('NRC sentiment analysis\nassigns words to emotions\n(fear, joy, etc).', x = 0.32, y = 0.8, angle = 0, size = 8, family = "Jua", fontface = "bold") + 
    cowplot::draw_text('The higher the score,\nthe more words were\nassigned to that emotion.', x = 0.72, y = 0.7, angle = 0, size = 7, family = "Jua", fontface = "bold") + 
    cowplot::draw_text('In other words,\nthere are more negative words\nused than positive words when\nthe news media talks about spiders', x = 0.72, y = 0.45, angle = 0, size = 6, family = "Jua", fontface = "bold")
  


ggsave('figs/nrc_plot.png', nrc_plot, bg='transparent', width = 10, height = 8)
ggsave('figs/nrc_plot.svg', nrc_plot, bg='transparent', width = 10, height = 8)
```

Separating spiders by medical relevance
```{r}
#| code-fold: true

#...............Getting medically relevant spiders...............

# get all unique species
unique_sp <- spider_news$Species %>% unique() %>% tm::stripWhitespace()

# subset for medically relevant species
medically_rel_sp <- unique_sp[
    (
    stringr::str_detect(unique_sp, "Loxosceles") == TRUE | 
    stringr::str_detect(unique_sp, "Sicarius") == TRUE | 
    stringr::str_detect(unique_sp, "Hexophthalma") == TRUE |
    stringr::str_detect(unique_sp, "Latrodectus") == TRUE | 
    stringr::str_detect(unique_sp, "Phoneutria") == TRUE | 
    stringr::str_detect(unique_sp, "Atrax") == TRUE | 
    stringr::str_detect(unique_sp, "Hadronyche") == TRUE | 
    stringr::str_detect(unique_sp, "Missulena") == TRUE
    ) & 
      
    is.na(unique_sp) == FALSE
  ]

# assign binary var for medically relevant or not
spider_news_weighted <- spider_news_weighted %>% 
                          mutate(medically_relevant = case_when(
                              Species %in% medically_rel_sp ~ 1,
                              .default = 0
                            
                              )
                            )
```

Data wrangling and preparation for hanging spider lolipop graph
```{r}
#| code-fold: true

# define function to calculate mode
get_mode <- function(x) {
  u <- unique(x)
  tab <- tabulate(match(x, u))
  u[tab == max(tab)]
}

# get mean bias by family
fam_med_counts <- spider_news_weighted %>% 
  group_by(Family) %>% 
  summarize(mean_bias = mean(Total_error), 
            medically_relevant = get_mode(medically_relevant), 
            count = n()) %>% 
  ungroup() %>% 
  mutate(x_placement = case_when(
              medically_relevant == 0 ~ seq(-0.55, 0.30, length.out = n()), 
              medically_relevant == 1 ~ seq(0.75, 1.35, length.out = n())
              )) %>%
  mutate(Family = tm::stripWhitespace(Family))

# filter for portion of species
spider_pnt_select <- filter(fam_med_counts, (medically_relevant == 0 & mean_bias > 3) | medically_relevant == 1) %>% 
  mutate(y_adjust = case_when(
    Family == 'Actinopodidae' ~ 1,
    .default = 0
  ), 
        x_adjust = case_when(
          Family == 'Atracidae' ~ -0.07, 
          Family %in% c('Ctenidae', 'Theridiidae') ~ 0.07, 
          .default = 0
        )
)

# get mean bias by medical relevance
med_rel_counts <- fam_med_counts %>% 
  group_by(medically_relevant) %>% 
  summarize(mean_bias = mean(mean_bias), 
            count = sum(count))

# set up df to plot spider legs
curve_params <- data.frame(
    x = spider_pnt_select  %>% select(x_placement) %>% slice(rep(1:n(), each = 8)), 
    x_add = data.frame(x_add = c(0.01, 0.025, 0.035, 0.05, -0.01, -0.025, -0.035, -0.05)) %>% slice(rep(row_number(), 1)), 
    y = spider_pnt_select %>% rename(y = mean_bias) %>% select(y) %>% slice(rep(1:n(), each = 8)), 
    y_mult = data.frame(y_mult = c(7.0-1.3, 6.9-1.3, 6.7-1.2, 6.2-0.8)) %>% slice(rep(row_number(), 2)), 
    y_end_mult = rep(5, nrow(spider_pnt_select)),
    curvature = data.frame(curvature = c(0.5,0.5,0.5,0.5,  -0.5,-0.5,-0.5,-0.5)) %>% slice(rep(row_number(), 1))
)
```

Building lolipop graph
```{r}
#| code-fold: true

med_plot <- ggplot() +
  
  # plot pointy bars
   geomwindmill::geom_windmill(
     data = med_rel_counts, 
     aes(
       x = medically_relevant, 
       y = count * 10e-3,
       group = medically_relevant
       ), 
     fill = "burlywood", 
     alpha = 0.8) + 
  
  # add bias counts
  ggplot2::annotate("text", 
                    x = c(0,1), 
                    y = 41, 
                    label = c(round(med_rel_counts$mean_bias[1], 2), 
                              round(med_rel_counts$mean_bias[2], 2)), 
                    family = "Gaegu", 
                    fontface = "bold",
                    size = 5.5) +
  # labeling for the prior
  ggplot2::annotate("text", 
                    x = c(0,1), 
                    y = 44, 
                    label = "~~~~Average Bias~~~~", 
                    family = "Gaegu", 
                    fontface = "bold",
                    size = 5.5) +
  
  # add points for lolipop chart 
   geom_point(
     data = filter(fam_med_counts, 
                   medically_relevant == 0 & mean_bias > 3), 
     aes(x_placement, mean_bias*5), size = 4) + 
   geom_point(
     data = filter(fam_med_counts, 
                   medically_relevant == 1), 
     aes(x_placement, mean_bias*5), size = 4) + 
   
   # segments for loliipop chart
   geom_segment(
     data = filter(fam_med_counts, 
                   medically_relevant == 0 & mean_bias > 3), 
     aes(
       x = x_placement, 
       xend = x_placement, 
       y = 0, 
       yend = mean_bias*5
       )) + 
   geom_segment(
     data = filter(fam_med_counts, 
                   medically_relevant == 1), 
     aes(
       x = x_placement, 
       xend = x_placement, 
       y = 0, 
       yend = mean_bias*5
       )) + 
  
####--------LEGS START HERE----------   

  geom_curve(
    data = filter(curve_params, curvature == 0.5),
    aes(
      x = x_placement+x_add,
      y = y*y_mult*0.98,
      xend = x_placement,
      yend = y*5,
    ),
    curvature = 0.5
  ) +
  geom_curve(
    data = filter(curve_params, curvature == -0.5),
    aes(
      x = x_placement+x_add,
      y = y*y_mult*0.98,
      xend = x_placement,
      yend = y*5,
    ),
    curvature = -0.5
  ) +

####--------END LEGS----------

  # family text labels
  geom_text(
    data = spider_pnt_select,
    aes(
      x = x_placement + x_adjust,
      y = mean_bias * (5.9 + y_adjust),
      label = Family,
      family = "Gaegu"
    )
  ) +

  # make continuous scale appear discrete
   scale_x_continuous(labels = c("","Not Medically Signficant", "", 
                                 "Medically Significant", ""), 
                      breaks = scales::extended_breaks()) + 
  
  # turn graph upside down
   scale_y_reverse(expand = c(0,0), 
                   limits = c(50, 0)) + 
  
  # allow drawing outside of plot area
   coord_cartesian(clip = "off") +
  
  # give color for bias
   scale_fill_gradient(high = 'brown2', low = 'brown4', breaks = NULL) + 
  
   labs(x = "", 
        y = "Number of News Articles by Family and Medical Relevance", 
        title = "Of the spiders in the news,\nwhich should we even be concerned about?") +
   theme(
         panel.background = element_rect(fill='transparent'), 
         plot.background = element_rect(fill='transparent', color=NA), 
         panel.grid = element_blank(), 
         axis.text = element_text(family = "Gaegu", 
                                  face = "bold",
                                  size = 15), 
         axis.title = element_text(family = "Gaegu"), 
         plot.title = element_text(family = "Jua", 
                                   size = 20, 
                                   hjust = 0.5))


med_plot

ggsave('figs/med_plot.png', med_plot, bg='transparent', width = 8, height = 9)
ggsave('figs/med_plot.svg', med_plot, bg='transparent', width = 8, height = 9)
```

Preparation and grabbing metrics for exposure therapy graph
```{r}
#| code-fold: true

# compile all relevant exposure thearapy data
full_tol <- data.frame(
  tol_score = c(spider_tolerance$FSQ_1_sum, spider_tolerance$FSQ_2_sum, spider_tolerance$FSQ_3_sum),
  participant = rep(spider_tolerance$participant, 3),
  trial = rep(c("1", "2", "3"), each = length(spider_tolerance$participant))
) %>% 
  mutate(score_bin = ntile(tol_score, 10))

# average by trial and low/high initial fear
group_tol <- data_frame(
  participant = rep(spider_tolerance$participant, 3), 
  score_tol = c(spider_tolerance$FSQ_1_sum, spider_tolerance$FSQ_2_sum, spider_tolerance$FSQ_3_sum), 
  score_bin = c(spider_tolerance$FSQ_1_sum, spider_tolerance$FSQ_2_sum, spider_tolerance$FSQ_3_sum) %>% ntile(2), 
  trial = rep(c("1", "2", "3"), each = length(spider_tolerance$participant))) %>% 
  group_by(score_bin, trial) %>%
  reframe(score_tol = mean(score_tol, na.rm = TRUE))


# get overall mean of all people over the three trials
full_tol_mean <- full_tol %>% 
  group_by(trial) %>% 
  summarize(tol_score = mean(tol_score, na.rm = TRUE)) %>% 
  ungroup()


  
# get percent change for low, high, and everyone
before_low <- group_tol %>% filter(trial == 1 & score_bin == 1) %>% select(score_tol) %>% as.numeric()
after_low <- group_tol %>% filter(trial == 3 & score_bin == 1) %>% select(score_tol) %>% as.numeric()
pct_change_low <- ((after_low - before_low)/before_low) * 100
  
before_high <- group_tol %>% filter(trial == 1 & score_bin == 2) %>% select(score_tol) %>% as.numeric()
after_high <- group_tol %>% filter(trial == 3 & score_bin == 2) %>% select(score_tol) %>% as.numeric()
pct_change_high <- ((after_high - before_high)/before_high) * 100

before_mean <- full_tol_mean %>% filter(trial == 1) %>% select(tol_score) %>% as.numeric()
after_mean <- full_tol_mean %>% filter(trial == 3) %>% select(tol_score) %>% as.numeric()
pct_change <- ((after_mean - before_mean)/before_mean) * 100


#| code-fold: true

# overall change for each subject
effect_df <- data_frame(
  before = spider_tolerance$FSQ_1_sum,
  after = spider_tolerance$FSQ_3_sum, 
  difference = after - before
) %>% 
  filter(is.na(difference) == FALSE)

# get overall pct benefit of therapy
decrease <- effect_df %>% filter(difference < 0) %>% nrow()
tot <- effect_df %>% nrow()

pct_benefit <- decrease/tot


```

Building exposure therapy graph
```{r}
#| code-fold: true

exposure_plot <- ggplot() +
  
  # mapping high and low lines
   geom_line(data = group_tol, 
             mapping = aes(trial, score_tol, 
                           group = score_bin, 
                           color = score_bin
                           ), 
             lwd = 2, 
             lty = 6
             ) + 
  # mapping mean line
  geom_line(data = full_tol_mean, 
            mapping = aes(factor(trial), 
                          tol_score, 
                          group = 1
                          ), 
            lwd = 3,
            color = 'navajowhite4'
            ) + 
  
  
  
  
  # high and low slopes
  geom_line(data = group_tol %>% mutate(
    score_tol = ifelse(trial == 2, NA, score_tol)) %>% 
      drop_na(), 
            mapping = aes(trial, score_tol, 
                           group = score_bin, 
                           color = score_bin
                           ), 
             lwd = 1, 
             lty = 4,
             alpha = 0.6
    ) + 
  
  # mean slope
  geom_line(data = full_tol_mean %>% mutate(
    tol_score = ifelse(trial == 2, NA, tol_score)) %>% 
      drop_na(), 
            mapping = aes(factor(trial), 
                          tol_score, 
                          group = 1
                          ), 
            lwd = 1, 
            lty = 4
    ) + 
  
  
  
  
   # high and low points
  geom_point(data = group_tol %>% mutate(
    score_tol = ifelse(trial == 2, NA, score_tol)) %>% 
      drop_na(), 
            mapping = aes(trial, score_tol, 
                           group = score_bin
                           ), 
    size = 5, 
    alpha = 0.5
    ) + 
  
  # mean points
  geom_point(data = full_tol_mean %>% mutate(
    tol_score = ifelse(trial == 2, NA, tol_score)) %>% 
      drop_na(), 
            mapping = aes(factor(trial), 
                          tol_score, 
                          group = 1
                          ), 
    size = 5, 
    alpha = 0.5
    ) + 
  
  
  
  
  
  
  # slope labels
  ggplot2::annotate("text", 
            x = c(2,2,2,
                  0.6,3.4,3.4), 
            y = c(38,57,76,
                  42,50,73
                         ), 
            label = c(
              paste0("Change: ", round(pct_change_low, 2), " %"), 
              paste0("Change: ", round(pct_change, 2), " %"), 
              paste0("Change: ", round(pct_change_high, 2), " %"),
              
              paste("Exposure therapy\nwas most\nlikely to \nhave a beneficial\neffect in individuals\nwith less fear\nto begin with."), 
              paste("But,\non average,\nexposure\ndecreases fear."), 
              paste("Individuals\nwith high\namounts of\narachnophobia\nwill likely remain fearful\nregardless of exposure.")
              ), 
            family = 'Jua', 
            angle = c(-7.517, -4.792, 1.362, 
                      0, 0, 0), 
            size = c(4,4,4,
                     4.5,4.5,4.5)
           ) + 
  
  
  
  
  
  # high and low num labels
  geom_text(data = group_tol %>% mutate(
    score_tol = ifelse(trial == 2, NA, score_tol)), 
            mapping = aes(factor(trial), score_tol, label = round(score_tol,2)), 
    nudge_y = 3, 
    size = 5, 
    family = "Jua") + 
  
  # mean num labels
  geom_text(data = full_tol_mean %>% mutate(
    tol_score = ifelse(trial == 2, NA, tol_score)), 
            mapping = aes(factor(trial), tol_score, label = round(tol_score,2)), 
    nudge_y = 3, 
    size = 5, 
    family = "Jua") + 
  
  labs(x = "Fear of Spiders Questionaire Trial", 
       y = "Mean Score", 
       title = "What can we do about it?", 
       subtitle = "Karner (2024) investigated the power of exposure therapy.", 
       caption = "____________________________________________________\nFSQ derived from Szymanski & Donohue (1995).\nTrials 1,2 and 3, done before, during, and after therapy respectively.") + 
  
  
  
  
  
  
  coord_cartesian(clip = "off", 
                  xlim = c(0.8,3.3)) +
  
  
  
  
  scale_color_gradient(high = "grey20", low = "lemonchiffon4", breaks = NULL) +
  
  
  

  theme_minimal() + 
  
  theme(panel.grid = element_blank(), 
        axis.title = element_text(family = 'Gaegu', 
                                  face = "bold", 
                                  size = 20), 
        axis.text = element_text(family = 'Gaegu', 
                                  face = "bold", 
                                  size = 20), 
        plot.title = element_text(hjust = 0.5,
                                  size = 30,
                                  family = 'Gaegu',
                                  face = "bold"),
        plot.caption = element_text(size = 15,
                                  family = 'Gaegu',
                                  face = "bold"), 
        plot.subtitle = element_text(hjust = 0.5,
                                  size = 18,
                                  family = 'Gaegu',
                                  face = "bold"))


ggsave('figs/exposure_plot.png', exposure_plot, bg='transparent', width = 10, height = 8)
ggsave('figs/exposure_plot.svg', exposure_plot, bg='transparent', width = 10, height = 8)
```



### Citations:

Stefano Mammola, Jagoba Malumbres-Olarte, Valeria Arabesky et al. The global spread of (mis)information on spiders, 28 February 2022, PREPRINT (Version 1) available at Research Square [https://doi.org/10.21203/rs.3.rs-1383492/v1]

Karner, A., Zhang, M., Lor, C. S., Steyrl, D., Götzendorfer, S. J., Weidt, S., ... & Scharnowski, F. (2024). The “SpiDa” dataset: self-report questionnaires and ratings of spider images from spider-fearful individuals. Frontiers in Psychology, 15, 1327367.

Szymanski, J., & O'Donohue, W. (1995). Fear of Spiders Questionnaire. Journal of behavior therapy and experimental psychiatry, 26(1), 31–34. https://doi.org/10.1016/0005-7916(94)00072-t