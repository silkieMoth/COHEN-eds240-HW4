---
title: "hw3_test"
format: html
---

```{r}
library(tidyverse)
library(syuzhet)
library(lubridate)
library(tm)
library(wordcloud)
library(tidytext)
library(textdata)


spider_news <- read_delim(here::here('data', 'Data_spider_news_global.csv'), delim = '\t')

population <- read_csv(here::here('data', 'world_population.csv'))

wealth <- read_csv(here::here('data', 'wealth', 'API_NY.GDP.MKTP.CD_DS2_en_csv_v2_76261.csv'), skip = 3)

deaths <- read_csv(here::here("data", "Morticd10_part6.csv"))

deaths2 <- read_csv(here::here("data", "NCHS_-_Leading_Causes_of_Death__United_States.csv"))

deaths3 <- read_csv(here::here("data", "Monthly_Counts_of_Deaths_by_Select_Causes__2014-2019.csv"))

deaths4 <- read_csv(here::here("data", "mortality_proj", "USA_sdr_long_idr.csv"))

spider_tolerance <- read_delim(here::here("data", "SpiDa", "SpiDa_filtered.csv"), delim = ";")
```

weigh the bias
```{r}
# spider_news_weighted <- spider_news 
# 
# 
# 
# spider_news_weighted[27:38] <- spider_news_weighted %>% 
#   
#   select(Bite:Photo_error) %>% 
#   
#   replace_na()


  
spider_news_weighted <- spider_news %>% 
  
  # impacts to bias score assigned arbitrarily
  mutate(
    
         # account for NAs
         Bite = replace_na(Bite, 1), 
         Death = replace_na(Death, 1), 
         Figure_species = replace_na(Figure_species, 1), 
         Figure_bite = replace_na(Figure_bite, 1), 
         
         # having an expert will reduce bias score
         Expert_arachnologist = replace_na(Expert_arachnologist, 1),
         Expert_doctor = replace_na(Expert_doctor, 1),
         Expert_others = replace_na(Expert_others, 1), 
         
         # sensationalism give sever bias penalty
         Sensationalism = replace_na(Sensationalism, 1),
         
         # multiple types of error will compound
         Taxonomic_error = replace_na(Taxonomic_error, 1),
         Venom_error = replace_na(Venom_error, 1),
         Anatomy_error = replace_na(Anatomy_error, 1),
         Photo_error = replace_na(Photo_error, 1),
    
         Bite = Bite * 1,
         Death = Death * 1, 
         Figure_species = Figure_species * 1, 
         Figure_bite = Figure_bite * 2, 
         
         # having an expert will reduce bias score
         Expert_arachnologist = Expert_arachnologist * -2,
         Expert_doctor = Expert_doctor * -1,
         Expert_others = Expert_others * -1, 
         
         # sensationalism give sever bias penalty
         Sensationalism = Sensationalism * 5,
         
         # multiple types of error will compound
         Taxonomic_error = case_when(
           Taxonomic_error != 0 ~ Taxonomic_error * 2, 
           Taxonomic_error == 0 | is.na(Taxonomic_error) == TRUE ~ 1),
         Venom_error = case_when(
           Venom_error != 0 ~ Venom_error * 2, 
           Venom_error == 0 | is.na(Venom_error) == TRUE ~ 1),
         Anatomy_error = case_when(
           Anatomy_error != 0 ~ Anatomy_error * 2, 
           Anatomy_error == 0 | is.na(Anatomy_error) == TRUE ~ 1),
         Photo_error = case_when(
           Photo_error != 0 ~ Photo_error * 2, 
           Photo_error == 0 | is.na(Photo_error) == TRUE ~ 1),
         
         Total_error = Bite + Death + Figure_species + (Taxonomic_error * Venom_error * Anatomy_error * Photo_error)
         
         )
```

clean and subset population dataset
```{r}
population_sub <- population %>% 
  janitor::clean_names() %>%
  select(country_territory, x2022_population) %>% 
  mutate(country_territory = case_match(country_territory, 
    'Bosnia and Herzegovina' ~ 'Bosnia', 
    'United Kingdom' ~ 'UK', 
    'United States' ~ 'USA', 
    .default = country_territory))
```

clean and subset wealth dataset
```{r}
wealth_sub <- wealth %>% 
  janitor::clean_names() %>% 
  select(country_name, x2023, x2022, x2014) %>% 
  mutate(country_name = case_match(country_name, 
    'Bosnia and Herzegovina' ~ 'Bosnia', 
    'Czechia' ~ 'Czech Republic', 
    'Egypt, Arab Rep.' ~ 'Egypt', 
    'Iran, Islamic Rep.' ~ 'Iran', 
    "Cote d'Ivoire" ~ 'Ivory Coast', 
    'Kyrgyz Republic' ~ 'Kyrgyzstan',
    'Russian Federation' ~ 'Russia', 
    'Korea, Rep.' ~ 'South Korea',
    'Turkiye' ~ 'Turkey', 
    'United Kingdom' ~ 'UK', 
    'United States' ~ 'USA', 
    'Venezuela, RB' ~ 'Venezuela',
    'Syrian Arab Republic' ~ 'Syria', 
    .default = country_name)) %>% 
  filter(country_name != 'Venezuela')

# NOTE: palestine, taiwan, venezuela omitted from world bank dataset

```


```{r}
spider_news_weighted %>% 
  group_by(Total_error) %>% 
  summarize(count = n()) %>% 
  ggplot(aes(x = as.factor(Total_error), y = count)) + 
  geom_col()
```
```{r}
spider_news_weighted %>% 
  group_by(Country_search) %>% 
  summarize(avg_bias = mean(Total_error)) %>% 
  filter(avg_bias > 5) %>% 
  na.omit() %>% 
  left_join(population_sub, by = join_by(Country_search == country_territory)) %>% 
  
  
  ggplot(aes(x = Country_search, y = sort(avg_bias))) + 
  geom_col() + 
  coord_flip() + 
  labs(title = 'Countries Most biased against spiders in the news', 
       y = 'Average Reporting Bias', 
       caption = "Limitations: wealth and population not accounted for in estimate,\nweights to different biases assigned arbitrarily") + 
  theme_minimal() + 
  theme(panel.grid = element_blank(),
        axis.title.y = element_blank())
```

```{r}
spider_news_weighted %>% 
  group_by(Country_search) %>% 
  na.omit() %>% 
  summarize(avg_bias = mean(Total_error)) %>%
  left_join(population_sub, by = join_by(Country_search == country_territory)) %>% mutate(bias_per_cap = avg_bias/(x2022_population / 10e6)) %>% 
  filter(bias_per_cap > 5) %>% 
  
  
  ggplot(aes(x = Country_search, y = sort(bias_per_cap))) +
  geom_col() +
  coord_flip() +
  labs(title = 'Countries Most biased against spiders in the news',
       y = 'Average Reporting Bias',
       caption = "Limitations: wealth and population not accounted for in estimate,\nweights to different biases assigned arbitrarily") +
  theme_minimal() +
  theme(panel.grid = element_blank(),
        axis.title.y = element_blank())
```

```{r}
df_gdp <- spider_news_weighted %>% 
  group_by(Country_search) %>% 
  na.omit() %>% 
  summarize(avg_bias = mean(Total_error)) %>%
  full_join(wealth_sub, by = join_by(Country_search == country_name)) %>% 
#  filter(is.na(x2023) == TRUE) %>% 
  filter(Country_search != 'Palestine' & Country_search != 'Taiwan') %>% 
  mutate(gdp = case_when(
    is.na(x2023) == FALSE ~ x2023, 
    is.na(x2023) == TRUE & is.na(x2022) == FALSE ~ x2022,
    is.na(x2023) == TRUE & is.na(x2022) == TRUE & is.na(x2014) == FALSE ~ x2014
  )) %>% 
  select(Country_search, avg_bias, gdp) %>%
  filter(is.na(avg_bias) == FALSE)


df_pop <- spider_news_weighted %>% 
  group_by(Country_search) %>% 
  na.omit() %>% 
  summarize(avg_bias = mean(Total_error)) %>%
  left_join(population_sub, by = join_by(Country_search == country_territory))

df_full <- left_join(df_gdp, df_pop, by = 'Country_search') %>% 
  select(-avg_bias.y) %>% 
  rename('avg_bias' = 'avg_bias.x',
         'population' = 'x2022_population', 
         'country' = 'Country_search')
```

```{r}
  ggplot() +
  geom_point(data = df_full, 
             mapping = aes(x = avg_bias, 
                           y = gdp, 
                           size = population), 
             show.legend = FALSE) +
  
  geom_point(data = subset(df_full, avg_bias > 8), 
             mapping = aes(x = avg_bias, 
                           y = gdp, 
                           size = population), 
             color = 'red', 
             show.legend = FALSE) + 
  geom_text(data = subset(df_full, avg_bias > 8), 
            mapping = aes(x = avg_bias, 
                          y = gdp, 
                          label = country), 
            size = 3, 
            position = position_jitter(0.5, 3e12, 20), 
            fontface = 'bold') + 
  
  geom_point(data = subset(df_full, gdp > 1e13), 
             mapping = aes(x = avg_bias, 
                           y = gdp, 
                           size = population), 
             color = 'yellow', 
             show.legend = FALSE) + 
  geom_text(data = subset(df_full, gdp > 1e13), 
            mapping = aes(x = avg_bias, 
                          y = gdp, 
                          label = country), 
            nudge_x = 0.7, 
            nudge_y = 2e12, 
            fontface = 'bold') + 
  geom_curve(aes(x = c(8.5, 9.3, 9.55, 11), 
                 y = c(-1e12, 2e12, -1.7e12, 2e12), 
                 xend = c(8.7, 9.05, 9.95, 10.75), 
                 yend = c(1e12, 0, 7e11, -3e11)), 
             curvature = c(-1.2), 
             lwd = 0.7, 
             arrow = arrow(type = 'closed', length = unit(0.2, 'cm'))) + 
#  coord_flip() +
  labs(title = 'Countries Most biased against spiders in the news,by GDP and population', 
       x = 'Average Reporting Bias', 
       y = 'GDP ($)',
       caption = "All GDPs from 2023, except Lebanon and Syria which were from 2022\nLimitations: weights to different biases assigned arbitrarily") +
  theme_minimal() +
  theme(panel.grid = element_blank())

```

```{r}
all_english <- spider_news %>% 
  filter(Language == 'English') %>% 
  select(Title) %>% as_vector() %>% VectorSource() %>% 
  SimpleCorpus()

all_english_senti <- all_english %>% 
  tm_map(tolower) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(removeNumbers) %>% 
  tm_map(removeWords, stopwords('english')) %>% 
#  tm_map(stemDocument) %>%
  tm_map(stripWhitespace)

tdm <- TermDocumentMatrix(all_english_senti) %>% as.matrix()

row_sums <- rowSums(tdm)
```

```{r}
sentiment_tib <- enframe(row_sums, name = "word", value = "count")
```

```{r}
ggplot(subset(sentiment_tib, count > 50), aes(word, sort(count))) +
  geom_col() +
  coord_flip()
```

```{r}
wordcloud(words = names(row_sums),
          freq = row_sums,
          max.words = 150,
          random.order = TRUE,
          min.freq = 5,
          colors = brewer.pal(8, 'Dark2'),
          scale = c(2.7, 0.8),
          rot.per = 0.7, 
          use.r.layout = FALSE)
```


```{r}
sentiment <- row_sums %>% 
  iconv() %>% 
  get_nrc_sentiment()
```

```{r}
sentiment2 = c()

for (i in row_sums) {
  
sentiment2 <- c(sentiment2, syuzhet::get_sent_values(i))
}
```


try affin sentiment anaylsis
* afinn style assigns whole number values to positive and negative
```{r}
# Convert to data frame (same as above)
afinn_prep_df <- all_english <- spider_news %>% 
  filter(Language == 'English') %>% 
  select(Title) %>% rename('text' = 'Title')

# Convert to lowercase (same as above)
afinn_prep_df$text <- tolower(afinn_prep_df$text)

# Remove punctuation (same as above)
afinn_prep_df$text <- gsub("[[:punct:]]", "", afinn_prep_df$text)

# Unnest the text into words (same as above)
text_words <- afinn_prep_df %>%
  tidytext::unnest_tokens(word, text)

# depluralize words
deplural <- text_words %>% 
  mutate(word = pluralize::singularize(word))



```



```{r}
# Load the AFINN lexicon
afinn <- get_sentiments("afinn")

# Join the text words with the AFINN lexicon
sentiment_analysis_afinn <- text_words %>%
  inner_join(afinn, by = "word")
```

touch up row_sums so it can be used in wordcloud
```{r}
# prepare new wordcloud df
wordcloud_prep <- data.frame(names(row_sums), as_tibble(row_sums)) %>% 
  rename(word = names.row_sums., frequency = value) %>%
  mutate(word = pluralize::singularize(word)) %>%
  group_by(word) %>% reframe(frequency = sum(frequency)) %>% 
  filter(!word %in% c("spider")) %>% 
  inner_join(afinn, by = "word")
```


```{r}
# Summarize sentiment scores
sentiment_summary_afinn <- sentiment_analysis_afinn %>%
  group_by(value) %>%
  summarise(count = n(), .groups = 'drop') %>%
  arrange(desc(value))
```


make wordcloud again
```{r}
wordcloud(words = wordcloud_prep$word,
          freq = wordcloud_prep$frequency,
#          max.words = 75,
          random.order = TRUE,
          min.freq = 5,
          colors = tail(brewer.pal(9, 'Purples'), 5),
          scale = c(2.7, 0.8),
          rot.per = 0, 
          use.r.layout = FALSE, 
          family = "Ink Free")

```


```{r}
extrafont::loadfonts("win", quiet = FALSE)

# Create a bar chart of sentiment scores for AFINN
afinn_plot <- ggplot(sentiment_summary_afinn, aes(x = as.factor(value), y = count, fill = as.factor(value))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  coord_radial(inner.radius = 0.4,
               r.axis.inside = TRUE, 
               start = -1.2*pi, end = 0.2*pi, 
               clip = "off") +
#  geom_text(data = unique(sentiment_summary_afinn)[1,], aes(label = "test", x = , y = 0.5), inherit.aes = FALSE) +
  labs(title = "Sentiment Analysis Using AFINN Lexicon",
       x = "Sentiment Score",
       y = "Count") +
  scale_fill_manual(values = RColorBrewer::brewer.pal(n = 8, name = 'RdBu')) + 
  theme_minimal() + 
  theme(
    axis.title = element_blank(), 
#    axis.title.x = element_text(angle = 307, hjust = -17, vjust = -4, size = 11, family = "Ink Free", face = "bold")
    plot.title = element_text(family = "Ink Free", face = "bold"), 
    axis.text.theta = element_text(family = "Ink Free", face = "bold")
  )


cowplot::ggdraw(afinn_plot) +
 cowplot::draw_text('Sentiment Score', x = 0.7, y = 0.32, angle = 307, size = 11, family = "Ink Free", fontface = "bold")
```

try nrc
```{r, fig.asp=1.5}
nrc <- get_sentiments("nrc")

# Join the text words with the NRC lexicon
sentiment_analysis_nrc <- text_words %>%
  inner_join(nrc, by = "word")

# Summarize sentiment counts
sentiment_summary_nrc <- sentiment_analysis_nrc %>%
  count(sentiment, sort = TRUE)
```

```{r}
# Create a bar chart of sentiment counts
nrc_plot <- ggplot(sentiment_summary_nrc, aes(x = reorder(sentiment, n), y = n*10^-3, fill = sentiment)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  coord_radial(inner.radius = 0.4,
             r.axis.inside = TRUE, 
             start = -1.2*pi, end = 0.2*pi) +
  labs(title = "Sentiment Analysis Using NRC Lexicon",
       x = "Sentiment",
       y = "Count") +
  theme_minimal() + 
  theme(
    axis.title = element_blank(), 
#    axis.title.x = element_text(angle = 307, hjust = -17, vjust = -4, size = 11, family = "Ink Free", face = "bold")
    plot.title = element_text(family = "Ink Free", face = "bold"), 
    axis.text.theta = element_text(family = "Ink Free", face = "bold")
  )


cowplot::ggdraw(nrc_plot) +
 cowplot::draw_text('Sentiment Score (10^3)', x = 0.7, y = 0.32, angle = 307, size = 11, family = "Ink Free", fontface = "bold")
#  cowplot::draw_plot_label('Sentiment Score (10^3)', x = 0.7, y = 0.5, angle = 307, size = 11, family = "Ink Free", fontface = "bold")

#ggsave("figs/nrc_temp.png")
```

# we'll do the same thing but we are going to separate by countries with high and low bias

## prepare words
```{r}
# high bias 
all_english_hb <- spider_news_weighted %>% 
  filter(Language == 'English', 
         Total_error > 3) %>% 
  select(Title) %>% as_vector() %>% VectorSource() %>% 
  SimpleCorpus()

all_english_hb_senti <- all_english_hb %>% 
  tm_map(tolower) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(removeNumbers) %>% 
  tm_map(removeWords, stopwords('english')) %>% 
#  tm_map(stemDocument) %>%
  tm_map(stripWhitespace)

tdm_hb <- TermDocumentMatrix(all_english_hb_senti) %>% as.matrix()

row_sums_hb <- rowSums(tdm_hb)

rm(all_english_hb, all_english_hb_senti, tdm_hb)



# low bias 
all_english_lb <- spider_news_weighted %>% 
  filter(Language == 'English', 
         Total_error <= 3) %>% 
  select(Title) %>% as_vector() %>% VectorSource() %>% 
  SimpleCorpus()

all_english_lb_senti <- all_english_lb %>% 
  tm_map(tolower) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(removeNumbers) %>% 
  tm_map(removeWords, stopwords('english')) %>% 
#  tm_map(stemDocument) %>%
  tm_map(stripWhitespace)

tdm_lb <- TermDocumentMatrix(all_english_lb_senti) %>% as.matrix()

row_sums_lb <- rowSums(tdm_lb)

rm(all_english_lb, all_english_lb_senti, tdm_lb)
```

## make wordcloud dfs
```{r}
# high bias
wordcloud_prep_hb <- data.frame(names(row_sums_hb), as_tibble(row_sums_hb)) %>% 
  rename(word = colnames(.)[[1]], frequency = colnames(.)[[2]]) %>%
  mutate(word = pluralize::singularize(word)) %>%
  group_by(word) %>% reframe(frequency = sum(frequency)) %>%
  filter(!word %in% c("spider")) %>%
  inner_join(afinn, by = "word")

# low bias
wordcloud_prep_lb <- data.frame(names(row_sums_lb), as_tibble(row_sums_lb)) %>% 
  rename(word = colnames(.)[[1]], frequency = colnames(.)[[2]]) %>%
  mutate(word = pluralize::singularize(word)) %>%
  group_by(word) %>% reframe(frequency = sum(frequency)) %>% 
  filter(!word %in% c("spider")) %>% 
  inner_join(afinn, by = "word")
```

## make wordclouds themselves
```{r}
# high bias
wordcloud(words = wordcloud_prep_hb$word,
          freq = wordcloud_prep_hb$frequency,
          random.order = TRUE,
          min.freq = 4,
          colors = tail(brewer.pal(9, 'Purples'), 5),
          scale = c(2.7, 0.8),
          rot.per = 0, 
          use.r.layout = FALSE, 
          family = "Ink Free")

#save.image("figs/wordcloud_temp.png")
```

```{r}

wordcloud(words = wordcloud_prep_lb$word,
          freq = wordcloud_prep_lb$frequency,
#          max.words = 75,
          random.order = TRUE,
          min.freq = 4,
          colors = tail(brewer.pal(9, 'Purples'), 5),
          scale = c(2.7, 0.8),
          rot.per = 0, 
          use.r.layout = FALSE, 
          family = "Ink Free")
```

# now make the nrc sentiment analysis

## nrc prep
```{r}
# get nrc sentiments
nrc <- get_sentiments("nrc")

# high bias

# Convert to data frame (same as above)
nrc_prep_hb_df <- all_english <- spider_news_weighted %>% 
  filter(Language == 'English', 
         Total_error > 2) %>% 
  select(Title) %>% rename('text' = 'Title')

# Convert to lowercase (same as above)
nrc_prep_hb_df$text <- tolower(nrc_prep_hb_df$text)

# Remove punctuation (same as above)
nrc_prep_hb_df$text <- gsub("[[:punct:]]", "", nrc_prep_hb_df$text)

# Unnest the text into words (same as above)
text_words_hb <- nrc_prep_hb_df %>%
  tidytext::unnest_tokens(word, text)

# depluralize words
text_words_hb <- text_words_hb %>% 
  mutate(word = pluralize::singularize(word))

# Join the text words with the NRC lexicon
sentiment_analysis_hb_nrc <- text_words_hb %>%
  inner_join(nrc, by = "word")

# Summarize sentiment counts
sentiment_summary_hb_nrc <- sentiment_analysis_hb_nrc %>%
  count(sentiment, sort = TRUE)


rm(nrc_prep_hb_df, text_words_hb, sentiment_analysis_hb_nrc)


# low bias

# Convert to data frame (same as above)
nrc_prep_lb_df <- all_english <- spider_news_weighted %>% 
  filter(Language == 'English' , 
         Total_error <= 2) %>% 
  select(Title) %>% rename('text' = 'Title')

# Convert to lowercase (same as above)
nrc_prep_lb_df$text <- tolower(nrc_prep_lb_df$text)

# Remove punctuation (same as above)
nrc_prep_lb_df$text <- gsub("[[:punct:]]", "", nrc_prep_lb_df$text)

# Unnest the text into words (same as above)
text_words_lb <- nrc_prep_lb_df %>%
  tidytext::unnest_tokens(word, text)

# depluralize words
text_words_lb <- text_words_lb %>% 
  mutate(word = pluralize::singularize(word))

# Join the text words with the NRC lexicon
sentiment_analysis_lb_nrc <- text_words_lb %>%
  inner_join(nrc, by = "word")

# Summarize sentiment counts
sentiment_summary_lb_nrc <- sentiment_analysis_lb_nrc %>%
  count(sentiment, sort = TRUE)

rm(nrc_prep_lb_df, text_words_lb, sentiment_analysis_lb_nrc)


```

## make graphs
```{r}
# high bias

# Create a bar chart of sentiment counts
nrc_plot <- ggplot(sentiment_summary_hb_nrc, aes(x = reorder(sentiment, n), y = n*10^-3, fill = sentiment)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  coord_radial(inner.radius = 0.4,
             r.axis.inside = TRUE, 
             start = -1.2*pi, end = 0.2*pi) +
  labs(title = "Sentiment Analysis Using NRC Lexicon",
       x = "Sentiment",
       y = "Count") +
  theme_minimal() + 
  theme(
    axis.title = element_blank(), 
#    axis.title.x = element_text(angle = 307, hjust = -17, vjust = -4, size = 11, family = "Ink Free", face = "bold")
    plot.title = element_text(family = "Ink Free", face = "bold"), 
    axis.text.theta = element_text(family = "Ink Free", face = "bold")
  )


cowplot::ggdraw(nrc_plot) +
 cowplot::draw_text('Sentiment Score (10^3)', x = 0.7, y = 0.32, angle = 307, size = 11, family = "Ink Free", fontface = "bold")
```

```{r}
# low bias

# Create a bar chart of sentiment counts
nrc_plot <- ggplot(sentiment_summary_lb_nrc, aes(x = reorder(sentiment, n), y = n*10^-3, fill = sentiment)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  coord_radial(inner.radius = 0.4,
             r.axis.inside = TRUE, 
             start = -1.2*pi, end = 0.2*pi) +
  labs(title = "Sentiment Analysis Using NRC Lexicon",
       x = "Sentiment",
       y = "Count") +
  theme_minimal() + 
  theme(
    axis.title = element_blank(), 
#    axis.title.x = element_text(angle = 307, hjust = -17, vjust = -4, size = 11, family = "Ink Free", face = "bold")
    plot.title = element_text(family = "Ink Free", face = "bold"), 
    axis.text.theta = element_text(family = "Ink Free", face = "bold")
  )


cowplot::ggdraw(nrc_plot) +
 cowplot::draw_text('Sentiment Score (10^3)', x = 0.7, y = 0.32, angle = 307, size = 11, family = "Ink Free", fontface = "bold")
```

just getting medically relevant spiders here
```{r}
unique_sp <- spider_news$Species %>% unique() %>% tm::stripWhitespace()

medically_rel_sp <- unique_sp[
    (
    stringr::str_detect(unique_sp, "Loxosceles") == TRUE | 
    stringr::str_detect(unique_sp, "Sicarius") == TRUE | 
    stringr::str_detect(unique_sp, "Hexophthalma") == TRUE |
    stringr::str_detect(unique_sp, "Latrodectus") == TRUE | 
    stringr::str_detect(unique_sp, "Phoneutria") == TRUE | 
    stringr::str_detect(unique_sp, "Atrax") == TRUE | 
    stringr::str_detect(unique_sp, "Hadronyche") == TRUE | 
    stringr::str_detect(unique_sp, "Missulena") == TRUE
    ) & 
      
    is.na(unique_sp) == FALSE
  ]


spider_news_weighted <- spider_news_weighted %>% 
                          mutate(medically_relevant = case_when(
                              Species %in% medically_rel_sp ~ 1,
                              .default = 0
                            
                              )
                            )
```

```{r, fig.asp=0.7}
# define function to calculate mode
get_mode <- function(x) {
  u <- unique(x)
  tab <- tabulate(match(x, u))
  u[tab == max(tab)]
}

spider_pnt_select <- filter(sp_med_counts, (medically_relevant == 0 & mean_bias > 3.5) | medically_relevant == 1)

# curve_params <- data.frame(
#   x = filter(sp_med_counts, (medically_relevant == 0 & mean_bias > 3.5) | medically_relevant == 1) %>% select(x_placement),
#   x_add = c(0.01, 0.025, 0.035, 0.05, -0.01, -0.025, -0.035, -0.05, 
#             0.01, 0.025, 0.035, 0.05, -0.01, -0.025, -0.035, -0.05), 
# #  xend = 
#   y = c(7.0, 6.9, 6.7, 6.2, 
#         7.0, 6.9, 6.7, 6.2,
#         7.0, 6.9, 6.7, 6.2,
#         7.0, 6.9, 6.7, 6.2), 
#   curvature = c(0.5,0.5,0.5,0.5,  -0.5,-0.5,-0.5,-0.5, 
#                 0.5,0.5,0.5,0.5,  -0.5,-0.5,-0.5,-0.5)
# )


# curve_params <- expand_grid(
#   x = filter(sp_med_counts, (medically_relevant == 0 & mean_bias > 3.5) | medically_relevant == 1) %>% select(x_placement),
#   x_add = c(0.01, 0.025, 0.035, 0.05, -0.01, -0.025, -0.035, -0.05),
#   y_mult = c(7.0, 6.9, 6.7, 6.2), 
#   curvature = c(0.5,0.5,0.5,0.5,  -0.5,-0.5,-0.5,-0.5),
# 
# 
# ) %>% 
#   group_by(x) %>% 
#   expand(x_add, .name_repair = make.names) %>% 
#   ungroup() %>% 
#   group_by(x, x_add) %>% 
#   expand(y)






# test <- expand.grid(split(
#   x = filter(sp_med_counts, (medically_relevant == 0 & mean_bias > 3.5) | medically_relevant == 1) %>% select(x_placement),
#     c(0.01, 0.025, 0.035, 0.05, -0.01, -0.025, -0.035, -0.05),
#   ))

# x = x_placement+0.01, 
#        xend = x_placement, 
#        y = mean_bias*5+2.8-0.8, 
#        yend = mean_bias*5
#        ))
# x = x_placement+0.025, 
#        xend = x_placement, 
#        y = mean_bias*5+2.7-0.8, 
#        yend = mean_bias*5
# x = x_placement+0.035, 
#        xend = x_placement, 
#        y = mean_bias*5+2.5-0.8, 
#        yend = mean_bias*5
# x = x_placement+0.05, 
#        xend = x_placement, 
#        y = mean_bias*5+2-0.8, 
#        yend = mean_bias*5


sp_med_counts <- spider_news_weighted %>% 
  group_by(Family) %>% 
  summarize(mean_bias = mean(Total_error), 
            medically_relevant = get_mode(medically_relevant)) %>% 
  ungroup() %>% 
  mutate(x_placement = case_when(
              medically_relevant == 0 ~ seq(-0.55, 0.30, length.out = n()), 
              medically_relevant == 1 ~ seq(0.75, 1.35, length.out = n())
              )) %>%
  mutate(Family = tm::stripWhitespace(Family))


med_rel_counts <- sp_med_counts %>% 
  group_by(medically_relevant) %>% 
  summarize(mean_bias = mean(mean_bias), 
            count = n())


curve_params <- data.frame(
    x = spider_pnt_select  %>% select(x_placement) %>% slice(rep(1:n(), each = 8)), 
    x_add = data.frame(x_add = c(0.01, 0.025, 0.035, 0.05, -0.01, -0.025, -0.035, -0.05)) %>% slice(rep(row_number(), 1)), 
    y = spider_pnt_select %>% rename(y = mean_bias) %>% select(y) %>% slice(rep(1:n(), each = 8)), 
    y_mult = data.frame(y_mult = c(7.0-1.3, 6.9-1.3, 6.7-1.2, 6.2-0.8)) %>% slice(rep(row_number(), 2)), 
    y_end_mult = rep(5, nrow(spider_pnt_select)),
    curvature = data.frame(curvature = c(0.5,0.5,0.5,0.5,  -0.5,-0.5,-0.5,-0.5)) %>% slice(rep(row_number(), 1))
)


med_plot <- ggplot() +
   geomwindmill::geom_windmill(
     data = med_rel_counts, 
     aes(
       x = medically_relevant, 
       y = count,, 
       group = medically_relevant
       ), 
     fill = "burlywood", 
     alpha = 0.5) + 
  
  
  ggplot2::annotate("text", 
                    x = c(0,1), 
                    y = 41, 
                    label = c(round(med_rel_counts$mean_bias[1], 2), 
                              round(med_rel_counts$mean_bias[2], 2)), 
                    family = "Mansalva", 
                    size = 5.5) +
  ggplot2::annotate("text", 
                    x = c(0,1), 
                    y = 44, 
                    label = "~~~~Average Bias~~~~", 
                    family = "Gaegu", 
                    size = 5.5) +
  
  # geom_text(data = med_rel_counts, 
  #           aes(
  #             x = medically_relevant, 
  #             y = count, 
  #             label = round(mean_bias, 2)
  #           ), 
  #           family = "Ink Free", 
  #           size = 5.5, 
  #           nudge_y = -1
  #           ) + 
  # geom_text(data = med_rel_counts, 
  #           aes(
  #             x = medically_relevant, 
  #             y = count, 
  #             label = "~~~~Mean Bias~~~~"
  #           ), 
  #           family = "Ink Free", 
  #           size = 5.5, 
  #           nudge_y = -4
  #           ) +
   
   
   geom_point(
     data = filter(sp_med_counts, 
                   medically_relevant == 0 & mean_bias > 3.5), 
     aes(x_placement, mean_bias*5), size = 4) + 
   geom_point(
     data = filter(sp_med_counts, 
                   medically_relevant == 1), 
     aes(x_placement, mean_bias*5), size = 4) + 
   
   
   geom_segment(
     data = filter(sp_med_counts, 
                   medically_relevant == 0 & mean_bias > 3.5), 
     aes(
       x = x_placement, 
       xend = x_placement, 
       y = 0, 
       yend = mean_bias*5
       )) + 
   geom_segment(
     data = filter(sp_med_counts, 
                   medically_relevant == 1), 
     aes(
       x = x_placement, 
       xend = x_placement, 
       y = 0, 
       yend = mean_bias*5
       )) + 
  
####--------LEGS START HERE----------   

  # geom_curve(
  #    data = filter(sp_med_counts,
  #                  medically_relevant == 1 | mean_bias > 3.5),
  #    aes(
  #      x = x_placement+0.01,
  #      xend = x_placement,
  #      y = mean_bias*5+2.8-0.8,
  #      yend = mean_bias*5
  #      )) +
  # geom_curve(
  #    data = filter(sp_med_counts,
  #                  medically_relevant == 1 | mean_bias > 3.5),
  #    aes(
  #      x = x_placement+0.025,
  #      xend = x_placement,
  #      y = mean_bias*5+2.7-0.8,
  #      yend = mean_bias*5
  #      )) +
  # geom_curve(
  #    data = filter(sp_med_counts,
  #                  medically_relevant == 1 | mean_bias > 3.5),
  #    aes(
  #      x = x_placement+0.035,
  #      xend = x_placement,
  #      y = mean_bias*5+2.5-0.8,
  #      yend = mean_bias*5
  #      )) +
  # geom_curve(
  #    data = filter(sp_med_counts,
  #                  medically_relevant == 1 | mean_bias > 3.5),
  #    aes(
  #      x = x_placement+0.05,
  #      xend = x_placement,
  #      y = mean_bias*5+2-0.8,
  #      yend = mean_bias*5
  #      )) +
  # 
  # 
  # geom_curve(
  #    data = filter(sp_med_counts,
  #                  medically_relevant == 1 | mean_bias > 3.5),
  #    aes(
  #      x = x_placement-0.01,
  #      xend = x_placement,
  #      y = mean_bias*5+2.8-0.8,
  #      yend = mean_bias*5
  #      ),
  #    curvature = -0.5) +
  # geom_curve(
  #    data = filter(sp_med_counts,
  #                  medically_relevant == 1 | mean_bias > 3.5),
  #    aes(
  #      x = x_placement-0.025,
  #      xend = x_placement,
  #      y = mean_bias*5+2.7-0.8,
  #      yend = mean_bias*5
  #      ),
  #    curvature = -0.5) +
  # geom_curve(
  #    data = filter(sp_med_counts,
  #                  medically_relevant == 1 | mean_bias > 3.5),
  #    aes(
  #      x = x_placement-0.035,
  #      xend = x_placement,
  #      y = mean_bias*5+2.5-0.8,
  #      yend = mean_bias*5
  #      ),
  #    curvature = -0.5) +
  # geom_curve(
  #    data = filter(sp_med_counts,
  #                  medically_relevant == 1 | mean_bias > 3.5),
  #    aes(
  #      x = x_placement-0.05,
  #      xend = x_placement,
  #      y = mean_bias*5+2-0.8,
  #      yend = mean_bias*5
  #      ),
  #    curvature = -0.5) +


  geom_curve( 
    data = filter(curve_params, curvature == 0.5),
    aes(
      x = x_placement+x_add,
      xend = x_placement,
      y = y*y_mult*0.95,
      yend = y*5,
    ), 
    curvature = 0.5
  ) + 
  geom_curve( 
    data = filter(curve_params, curvature == -0.5),
    aes(
      x = x_placement+x_add,
      xend = x_placement,
      y = y*y_mult*0.95,
      yend = y*5,
    ), 
    curvature = -0.5
  ) + 
  
  
   
  #  ggplot2::annotate("rect", 
  #                    xmin = c(0.85-0.05,0.9,0.9) - 0.2, xmax = c(1.25+0.3,0.95,0.95) - 0.2, ymin = c(30,31,33), ymax = c(35,32,34),
  #                    fill = c(NA, alpha('brown4', 0.5), alpha('brown2', 0.5)), color = "grey50") + 
  # ggplot2::annotate("text", 
  #                    x = c(0.95+0.27,0.95+0.23) - 0.2, y = c(31+0.3,33+0.3) + 0.1, 
  #                   label = c("Not Medically Significant", "Medically Significant"), 
  #                   size = 3, 
  #                   family = "Ink Free", 
  #                   fontface = "bold") +
   scale_x_continuous(labels = c("","Not Medically Signficant", "", "Medically Significant", ""), 
                      #limits = c(-1, 5), 
                      #n.breaks = 5, 
                      breaks = scales::extended_breaks()) + 
   scale_y_reverse(expand = c(0,0), 
                   limits = c(50, 0)) + 
   coord_cartesian(clip = "off") +
   scale_fill_gradient(high = 'brown2', low = 'brown4', breaks = NULL) + 
   labs(x = "", 
        y = "Number of Species/Family", 
        title = "Of the spiders in the news,\nwhich should we even be concerned about?") +
   theme(
     panel.background = element_blank(), 
     # panel.grid.major.x = element_line(color = alpha("chocolate4", 0.3), 
     #                           linewidth = 1), 
     #panel.grid = element_blank(),
     #panel.background = element_rect(fill = alpha("burlywood", 0.3)),
         axis.text = element_text(family = "Indie Flower", 
                                  face = "bold",
                                  size = 15), 
         axis.title = element_text(family = "Indie Flower"), 
         plot.title = element_text(family = "Jua", 
                                   size = 20, 
                                   hjust = 0.5))

# for (i in 1:nrow(curve_params)){
#   med_plot <- med_plot + geom_curve(
#     data = filter(sp_med_counts, medically_relevant == 0 & mean_bias > 3.5),
#     aes(x = x_placement+curve_params$x[i], 
#         xend = x_placement, 
#         y = mean_bias*curve_params$y[i],
#         yend = mean_bias*5),
#     curvature = curve_params$curvature[i]
#   )
# }



med_plot
```

chance of dying in the us... (per 100k people)
* from a venomous spider: 0.000078% (7.8)
* all venomous animals: 0.000795% (79.5)
* all nonvenomous animals: 0.001213% (121.3)
* from homicide: 0.09121240% (9121.240)
* from a car accident: 0.09637434% (9637.434)
* from cancer: 3.382527% (338252.7)

citations:
Centers for Disease Control and Prevention, National Center for Health Statistics. National Vital Statistics System, Mortality 2018-2023 on CDC WONDER Online Database, released in 2024. Data are from the Multiple Cause of Death Files, 2018-2023, as compiled from data provided by the 57 vital statistics jurisdictions through the Vital Statistics Cooperative Program. Accessed at http://wonder.cdc.gov/ucd-icd10-expanded.html on Mar 14, 2025 9:42:37 AM

Forrester, J. A., Holstege, C. P., & Forrester, J. D. (2012). Fatalities from venomous and nonvenomous animals in the United States (1999–2007). Wilderness & environmental medicine, 23(2), 146-152.

```{r}
deaths_agg <- deaths %>% 
  mutate(total_deaths = rowSums(across(Deaths1:Deaths26), na.rm = TRUE)) %>%
   group_by(Cause) %>% 
   reframe(total = sum(total_deaths)) %>% 
  # arrange(desc(total)) %>% 
  # filter(Cause == 'A138')
   filter(grepl("X8", Cause))

deaths_agg 

#unique(deaths_agg$Cause) %>% as_data_frame() %>% filter(grepl("E8", value))
```

```{r}
deaths2 %>% 
  filter(State == "United States") %>% 
  group_by(`113 Cause Name`) %>% 
  summarize(Deaths = sum(Deaths), 
            death_100k = Deaths/100000)
```

```{r}
deaths3 %>% 
  select(-Jurisdiction.of.Occurrence, -Month, -Year) %>% 
#  group_by(Year) %>% 
  summarize(across(everything(), ~ mean(.x, na.rm = TRUE))) %>% 
  pivot_longer(cols = everything(),
               names_to = "cause", 
               values_to = "death_tot") %>% 
  mutate(death_100k = death_tot/100000)
```

```{r}
deaths4 %>% 
  select(cause, SDR2013) %>% 
  group_by(cause) %>% 
  reframe(std_death_tot = sum(SDR2013), 
            death_100k = std_death_tot/100000) %>% 
  slice(48:75) %>% 
  summarize(cause = "cancer", 
            std_death_tot = sum(std_death_tot), 
            death_100k = sum(death_100k))
```

```{r}
spider_tolerance %>% 
  select(participant | (ends_with("sum") & starts_with("FSQ"))) %>% 
  # pivot_wider(names_from = participant, 
  #             names_prefix = "",
  #             values_from = c(FSQ_1_sum), 
  #             id_cols = c(FSQ_2_sum, FSQ_3_sum),
  #             id_expand = TRUE) 
  
  ggplot(aes(x = FSQ_1_sum)) + 
           geom_path()
```
```{r}
spider_tolerance %>% 
  select(participant | (ends_with("sum") & starts_with("FSQ")))
```

```{r}
spider_tol_wide <- c() %>% rbind(spider_tolerance$participant, 
                                 spider_tolerance$FSQ_1_sum, 
                                 spider_tolerance$FSQ_2_sum, 
                                 spider_tolerance$FSQ_3_sum) %>% 
  as_data_frame() %>%
   cbind(rows = c("participants", "FSQ_1_sum","FSQ_2_sum", "FSQ_3_sum")) %>% 
  column_to_rownames("rows")


plot_data_column <- function(data, column) {
    ggplot(data, aes_string(x = column)) +
        geom_histogram(fill = "lightgreen") +
        xlab(column)
}

myplots <- lapply(colnames(data2), plot_data_column, data = data2)

full_tol <- data.frame(
  tol_score = c(spider_tolerance$FSQ_1_sum, spider_tolerance$FSQ_2_sum, spider_tolerance$FSQ_3_sum),
  participant = rep(spider_tolerance$participant, 3),
  trial = rep(c("1", "2", "3"), each = length(spider_tolerance$participant))
) %>% 
  mutate(score_bin = ntile(tol_score, 10))

group_tol <- data_frame(
  participant = rep(spider_tolerance$participant, 3), 
  score_tol = c(spider_tolerance$FSQ_1_sum, spider_tolerance$FSQ_2_sum, spider_tolerance$FSQ_3_sum), 
  score_bin = c(spider_tolerance$FSQ_1_sum, spider_tolerance$FSQ_2_sum, spider_tolerance$FSQ_3_sum) %>% ntile(2), 
  trial = rep(c("1", "2", "3"), each = length(spider_tolerance$participant))) %>% 
  group_by(score_bin, trial) %>%
  reframe(score_tol = mean(score_tol, na.rm = TRUE))

full_tol_mean <- full_tol %>% 
  group_by(trial) %>% 
  summarize(tol_score = mean(tol_score, na.rm = TRUE)) %>% 
  ungroup()


  

before_low <- group_tol %>% filter(trial == 1 & score_bin == 1) %>% select(score_tol) %>% as.numeric()
after_low <- group_tol %>% filter(trial == 3 & score_bin == 1) %>% select(score_tol) %>% as.numeric()
pct_change_low <- ((after_low - before_low)/before_low) * 100
  
before_high <- group_tol %>% filter(trial == 1 & score_bin == 2) %>% select(score_tol) %>% as.numeric()
after_high <- group_tol %>% filter(trial == 3 & score_bin == 2) %>% select(score_tol) %>% as.numeric()
pct_change_high <- ((after_high - before_high)/before_high) * 100

before_mean <- full_tol_mean %>% filter(trial == 1) %>% select(tol_score) %>% as.numeric()
after_mean <- full_tol_mean %>% filter(trial == 3) %>% select(tol_score) %>% as.numeric()
pct_change <- ((after_mean - before_mean)/before_mean) * 100
```

```{r}
effect_df <- data_frame(
  before = spider_tolerance$FSQ_1_sum,
  after = spider_tolerance$FSQ_3_sum, 
  difference = after - before
) %>% 
#  mutate(difference = replace_na(difference, 999))
  filter(is.na(difference) == FALSE)

decrease <- effect_df %>% filter(difference < 0) %>% nrow()
tot <- effect_df %>% nrow()

pct_benefit <- decrease/tot


```

```{r}
ggplot() +
  
  # mapping high and low lines
   geom_line(data = group_tol, 
             mapping = aes(trial, score_tol, 
                           group = score_bin, 
                           color = score_bin
                           ), 
             lwd = 2, 
             lty = 6
             ) + 
  # mapping mean line
  geom_line(data = full_tol_mean, 
            mapping = aes(factor(trial), 
                          tol_score, 
                          group = 1
                          ), 
            lwd = 3,
            color = 'navajowhite4'
            ) + 
  
  
  
  
  # high and low slopes
  geom_line(data = group_tol %>% mutate(
    score_tol = ifelse(trial == 2, NA, score_tol)) %>% 
      drop_na(), 
            mapping = aes(trial, score_tol, 
                           group = score_bin, 
                           color = score_bin
                           ), 
             lwd = 1, 
             lty = 4,
             alpha = 0.6
    ) + 
  
  # mean slope
  geom_line(data = full_tol_mean %>% mutate(
    tol_score = ifelse(trial == 2, NA, tol_score)) %>% 
      drop_na(), 
            mapping = aes(factor(trial), 
                          tol_score, 
                          group = 1
                          ), 
            lwd = 1, 
            lty = 4
    ) + 
  
  
  
  
   # high and low points
  geom_point(data = group_tol %>% mutate(
    score_tol = ifelse(trial == 2, NA, score_tol)) %>% 
      drop_na(), 
            mapping = aes(trial, score_tol, 
                           group = score_bin
                           ), 
    size = 5, 
    alpha = 0.5
    ) + 
  
  # mean points
  geom_point(data = full_tol_mean %>% mutate(
    tol_score = ifelse(trial == 2, NA, tol_score)) %>% 
      drop_na(), 
            mapping = aes(factor(trial), 
                          tol_score, 
                          group = 1
                          ), 
    size = 5, 
    alpha = 0.5
    ) + 
  
  
  
  
  
  
  # slope labels
  ggplot2::annotate("text", 
            x = c(2,2,2,
                  0.6,3.4,3.4), 
            y = c(38,57,76,
                  42,50,73
                         ), 
            label = c(
              paste0("Change: ", round(pct_change_low, 2), " %"), 
              paste0("Change: ", round(pct_change, 2), " %"), 
              paste0("Change: ", round(pct_change_high, 2), " %"),
              
              paste("Exposure therapy\nwas most\nlikely to \nhave a beneficial\neffect in individuals\nwith less fear\nto begin with."), 
              paste("But,\non average,\nexposure\ndecreases fear."), 
              paste("Individuals\nwith high\namounts of\narachnophobia\nwill likely remain fearful\nregardless of exposure.")
              ), 
            family = 'Jua', 
            angle = c(-7.517, -4.792, 1.362, 
                      0, 0, 0), 
            size = c(4,4,4,
                     3.5,3.5,3.5)
           ) + 
  
  
  
  
  
  # high and low num labels
  geom_text(data = group_tol %>% mutate(
    score_tol = ifelse(trial == 2, NA, score_tol)), 
            mapping = aes(factor(trial), score_tol, label = round(score_tol,2)), 
    nudge_y = 3, 
    size = 5, 
    family = "Jua") + 
  
  # mean num labels
  geom_text(data = full_tol_mean %>% mutate(
    tol_score = ifelse(trial == 2, NA, tol_score)), 
            mapping = aes(factor(trial), tol_score, label = round(tol_score,2)), 
    nudge_y = 3, 
    size = 5, 
    family = "Jua") + 
  
  labs(x = "Fear of Spiders Questionaire Trial", 
       y = "Mean Score", 
       title = "What can we do about it?", 
       subtitle = "Karner (2024) investigated the power of exposure therapy.", 
       caption = "____________________________________________________\nFSQ derived from Szymanski & Donohue (1995).\nTrials 1,2 and 3, done before, during, and after therapy respectively.") + 
  
  
  
  
  
  
  coord_cartesian(clip = "off", 
                  xlim = c(0.8,3.3)) +
  
  
  
  
  scale_color_gradient(high = "grey20", low = "lightblue3", breaks = NULL) +
  
  
  

  theme_minimal() + 
  
  theme(panel.grid = element_blank(), 
        axis.title = element_text(family = 'Gaegu', 
                                  face = "bold", 
                                  size = 20), 
        axis.text = element_text(family = 'Gaegu', 
                                  face = "bold", 
                                  size = 20), 
        plot.title = element_text(hjust = 0.5,
                                  size = 30,
                                  family = 'Gaegu',
                                  face = "bold"),
        plot.caption = element_text(size = 15,
                                  family = 'Gaegu',
                                  face = "bold"), 
        plot.subtitle = element_text(hjust = 0.5,
                                  size = 18,
                                  family = 'Gaegu',
                                  face = "bold"))
```

Karner, A., Zhang, M., Lor, C. S., Steyrl, D., Götzendorfer, S. J., Weidt, S., ... & Scharnowski, F. (2024). The “SpiDa” dataset: self-report questionnaires and ratings of spider images from spider-fearful individuals. Frontiers in Psychology, 15, 1327367.